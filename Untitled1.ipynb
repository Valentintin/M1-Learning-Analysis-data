{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5660276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(threshold=10000,suppress=True)\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cf91d5c-3114-4c8f-bd72-06fa5c442130",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('credit_scoring.csv', sep=';')\n",
    "array = data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90b24fe-5187-48c0-bdde-ca489367ce0d",
   "metadata": {},
   "source": [
    "### transformer votre jeu de données issue de pandas qui sera de type Data Frame en numpy Array  (c.f. values) et séparer ensuite les variables caractéristiques de la variable à prédire (status) en deux tableaux différents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74033f27-5272-486a-bbe3-d247698b0204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de l'échantillon : 4375\n",
      "Pourcentage d'exemples positifs : 72.20571428571428%\n",
      "Pourcentage d'exemples négatifs : 27.794285714285717%\n"
     ]
    }
   ],
   "source": [
    "def analyse_data (data:np.array, label:np.array) :\n",
    "    taille_echantillon = data.shape[0]\n",
    "    pourcentage_positif = (label == 1).sum() / taille_echantillon * 100\n",
    "    pourcentage_negatif = (label == 0).sum() / taille_echantillon * 100\n",
    "    print(f\"Taille de l'échantillon : {taille_echantillon}\")\n",
    "    print(f\"Pourcentage d'exemples positifs : {pourcentage_positif}%\")\n",
    "    print(f\"Pourcentage d'exemples négatifs : {pourcentage_negatif}%\")\n",
    "\n",
    "X = array[:, :-1]  # Toutes les colonnes sauf la dernière\n",
    "label = array[:, -1]   # Dernière colonne\n",
    "analyse_data(X, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e95cfd0-0250-46ef-a305-586e00b094c4",
   "metadata": {},
   "source": [
    "### Analyser les propriétés de vos données : taille de l’échantillon (c.f. shape), pourcentage d’exemples positifs et négatifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df944514-f795-42c4-ab09-4c68d4cf89d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de l'échantillon : 2187\n",
      "Pourcentage d'exemples positifs : 72.0164609053498%\n",
      "Pourcentage d'exemples négatifs : 27.983539094650205%\n",
      "Taille de l'échantillon : 2188\n",
      "Pourcentage d'exemples positifs : 72.39488117001828%\n",
      "Pourcentage d'exemples négatifs : 27.605118829981716%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, label, test_size=0.5, random_state=1)\n",
    "analyse_data(X_train, y_train)\n",
    "analyse_data(X_test, y_test)\n",
    "# Faire ca après\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "# X_train_scaled = scaler.transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02434eb-2705-49c7-b285-2e0663ee92a3",
   "metadata": {},
   "source": [
    "### Pour éviter d’avoir un résultat biaisé du classifieur que nous allons construire, séparer les données en deux parties (de taille 50% chacune) une dite d’apprentissage qui servira à l’apprentissage du classifieur  et  l’autre  dite  de  test  qui  servira  à  son  évaluation  (c.f. train_test_split  avec  un random_state=1)\n",
    "Nous pouvons voir que le split donne des résultats coérant. (Teste sans normalisation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d46a6a92-94b6-4402-9041-546383a86883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(40, 20), random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(40, 20), random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(40, 20), random_state=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# arbre de désision\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# K voisin plus proche\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(X_train, y_train)\n",
    "\n",
    "# MultilayerPerceptron\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(40, 20), random_state=1)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "# Fin des entrainement des différents algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4399141-aa5a-4611-85f9-0782272989cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision tree confuse\n",
      "[[ 337  267]\n",
      " [ 336 1248]]\n",
      "decision tree- Accuracy: 72.44%\n",
      "decision tree - Precision: 82.38%\n",
      "decision tree - Recall: 78.79% \n",
      "\n",
      "neigh confuse\n",
      "[[ 189  415]\n",
      " [ 187 1397]]\n",
      "neigh- Accuracy: 72.49%\n",
      "neigh - Precision: 77.10%\n",
      "neigh - Recall: 88.19% \n",
      "\n",
      "MultilayerPerceptron confuse\n",
      "[[ 323  281]\n",
      " [ 351 1233]]\n",
      "MultilayerPerceptron- Accuracy: 71.12%\n",
      "MultilayerPerceptron - Precision: 81.44%\n",
      "MultilayerPerceptron - Recall: 77.84% \n",
      "\n",
      "knn is best accuracy with a score of 0.7248628884826326\n",
      "arbre is best precision with a score of 0.8237623762376237\n",
      "knn is best recall with a score of 0.8819444444444444\n"
     ]
    }
   ],
   "source": [
    "def show_predictions_result(pred, true, label=\"methode\"):\n",
    "    confuse_mlp = confusion_matrix(true, pred)\n",
    "    print(f\"{label} confuse\")\n",
    "    print(confuse_mlp)\n",
    "    accuracy = accuracy_score(true, pred)\n",
    "    precision = precision_score(true, pred)\n",
    "    recall = recall_score(true, pred)\n",
    "    \n",
    "    print(f\"{label}- Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"{label} - Precision: {precision * 100:.2f}%\")\n",
    "    print(f\"{label} - Recall: {recall * 100:.2f}% \\n\")\n",
    "    return {\"accuracy\":accuracy, \"precision\":precision, \"recall\":recall}\n",
    "\n",
    "clf_predictions = clf.predict(X_test)\n",
    "clf_res = show_predictions_result(clf_predictions, y_test, \"decision tree\")\n",
    "# Mesures de performance Pour l'abr de désision\n",
    "\n",
    "neigh_predictions = neigh.predict(X_test)\n",
    "neigh_res = show_predictions_result(neigh_predictions, y_test, \"neigh\")\n",
    "# Mesures de performance\n",
    "\n",
    "mlp_predictions = mlp_model.predict(X_test)\n",
    "# Mesures de performance de MultilayerPerceptron\n",
    "mlp_res = show_predictions_result(mlp_predictions, y_test, \"MultilayerPerceptron\")\n",
    "\n",
    "def compare (arbre, knn, multilayerPerceptron):\n",
    "    best_model, best_accuracy = max(\n",
    "        ((\"knn\", knn[\"accuracy\"]), (\"arbre\", arbre[\"accuracy\"]), (\"multilayerPerceptron\", multilayerPerceptron[\"accuracy\"])),\n",
    "        key=lambda x: x[1]\n",
    "    )\n",
    "    print(f\"{best_model} is best accuracy with a score of {best_accuracy}\")\n",
    "    best_model, best_precision = max(\n",
    "        ((\"knn\", knn[\"precision\"]), (\"arbre\", arbre[\"precision\"]), (\"multilayerPerceptron\", multilayerPerceptron[\"precision\"])),\n",
    "        key=lambda x: x[1]\n",
    "    )\n",
    "    print(f\"{best_model} is best precision with a score of {best_precision}\")\n",
    "    best_model, best_recall = max(\n",
    "        ((\"knn\", knn[\"recall\"]), (\"arbre\", arbre[\"recall\"]), (\"multilayerPerceptron\", multilayerPerceptron[\"recall\"])),\n",
    "        key=lambda x: x[1]\n",
    "    )\n",
    "    print(f\"{best_model} is best recall with a score of {best_recall}\")\n",
    "\n",
    "compare(clf_res, neigh_res, mlp_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e33efc4-3fb9-444e-9d9c-b8966f126bd4",
   "metadata": {},
   "source": [
    "### Apprentissage  et  évaluation  de  modèles  :  Utiliser  ensuite  sur  votre  jeu  de  données  les  algorithmes d’apprentissage supervisé suivants : \n",
    "- Un arbre CART (random_state=1) \n",
    "- k-plus-proches-voisins avec k=5 \n",
    "- MultilayerPerceptron à deux couches de tailles respectives 40 et 20 et random_state=1\n",
    "Nous avons entrainner nos modèles, sans normaliser nos donné voici le résultat. :\n",
    "decision tree confuse\n",
    "[[ 315  289]\n",
    " [ 323 1261]]\n",
    "decision tree- Accuracy: 72.03%\n",
    "decision tree - Precision: 81.35%\n",
    "decision tree - Recall: 79.61%\n",
    "\n",
    "neigh confuse\n",
    "[[ 189  415]\n",
    " [ 187 1397]]\n",
    "neigh- Accuracy: 72.49%\n",
    "neigh - Precision: 77.10%\n",
    "neigh - Recall: 88.19% \n",
    "\n",
    "MultilayerPerceptron confuse\n",
    "[[ 323  281]\n",
    " [ 351 1233]]\n",
    "MultilayerPerceptron- Accuracy: 71.12%\n",
    "MultilayerPerceptron - Precision: 81.44%\n",
    "MultilayerPerceptron - Recall: 77.84% \n",
    "\n",
    "Rappel :\n",
    "Taille de l'échantillon : 4375\n",
    "Pourcentage d'exemples positifs : 72.20571428571428%\n",
    "Pourcentage d'exemples négatifs : 27.794285714285717%\n",
    "Nous remarquons que nos algorithme sont a peinne égal aux hasard.\n",
    "C'est a dire très mauvais. Cela semble logique car nos données non pas encore été normaliser. nous remarquons aussi que Les algo ne sont bien parametrès. (Ex: arbre de prédiction over fiting)\n",
    "\n",
    "Dans notre cas la banque préfèrera ne pas avoir préter a quelqu'un qui aurais put rembourser, plus tôt que de préter a quelqu'un qui ne va pas pouvoir rembourser. C'est a dire nous préfererons, le critère pressision soit les FN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "639c9644-a103-40ed-bfc5-a160c236e471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.12417212  0.94505019  0.92225348 ... -0.28136029 -0.50556494\n",
      "  -0.98717625]\n",
      " [ 1.10300637  0.94505019  0.92225348 ... -0.28136029 -0.07976109\n",
      "   0.31890958]\n",
      " [ 0.2465264  -1.05814486 -0.71960165 ... -0.28136029  2.04925815\n",
      "   2.45336265]\n",
      " ...\n",
      " [-0.97701642 -1.05814486 -1.54052921 ... -0.28136029 -1.14427071\n",
      "  -0.79898408]\n",
      " [-0.97701642  0.94505019  0.10132592 ... -0.28136029 -1.03781975\n",
      "  -1.46328636]\n",
      " [-0.36524501 -1.05814486  0.92225348 ...  0.54000599  0.66539564\n",
      "   0.30604174]]\n",
      "decision tree confuse\n",
      "[[ 323  281]\n",
      " [ 329 1255]]\n",
      "decision tree- Accuracy: 72.12%\n",
      "decision tree - Precision: 81.71%\n",
      "decision tree - Recall: 79.23% \n",
      "\n",
      "neigh confuse\n",
      "[[ 282  322]\n",
      " [ 216 1368]]\n",
      "neigh- Accuracy: 75.41%\n",
      "neigh - Precision: 80.95%\n",
      "neigh - Recall: 86.36% \n",
      "\n",
      "MultilayerPerceptron confuse\n",
      "[[ 334  270]\n",
      " [ 225 1359]]\n",
      "MultilayerPerceptron- Accuracy: 77.38%\n",
      "MultilayerPerceptron - Precision: 83.43%\n",
      "MultilayerPerceptron - Recall: 85.80% \n",
      "\n",
      "multilayerPerceptron is best accuracy with a score of 0.7737659963436929\n",
      "multilayerPerceptron is best precision with a score of 0.8342541436464088\n",
      "knn is best recall with a score of 0.8636363636363636\n"
     ]
    }
   ],
   "source": [
    "# Normalisation des données\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_train_scaled_trans = scaler.transform(X)\n",
    "\n",
    "print (X_train_scaled_trans)\n",
    "\n",
    "X_norm_train, X_norm_test, y_norm_train, y_norm_test = train_test_split(X_train_scaled_trans, label, test_size=0.5, random_state=1)\n",
    "# arbre de désision\n",
    "clf_norm = tree.DecisionTreeClassifier()\n",
    "clf_norm.fit(X_norm_train, y_train)\n",
    "# K voisin plus proche\n",
    "neigh_norm = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh_norm.fit(X_norm_train, y_train)\n",
    "# MultilayerPerceptron\n",
    "mlp_model_norm = MLPClassifier(hidden_layer_sizes=(40, 20), random_state=1)\n",
    "mlp_model_norm.fit(X_norm_train, y_train)\n",
    "\n",
    "clf_norm_predictions = clf_norm.predict(X_norm_test)\n",
    "clf_norm_res = show_predictions_result(clf_norm_predictions, y_test, \"decision tree\")\n",
    "# Mesures de performance Pour l'abr de désision\n",
    "\n",
    "neigh_norm_predictions = neigh_norm.predict(X_norm_test)\n",
    "neigh_norm_res = show_predictions_result(neigh_norm_predictions, y_test, \"neigh\")\n",
    "# Mesures de performance\n",
    "\n",
    "mlp_norm_predictions = mlp_model_norm.predict(X_norm_test)\n",
    "# Mesures de performance de MultilayerPerceptron\n",
    "mlp_norm_res = show_predictions_result(mlp_norm_predictions, y_test, \"MultilayerPerceptron\")\n",
    "\n",
    "compare(clf_norm_res, neigh_norm_res, mlp_norm_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a764c6-31c7-400f-92fa-5313469c5984",
   "metadata": {},
   "source": [
    "### Normalisation  des  variables  continues  : \n",
    "Nos nouveaux résultat :\n",
    "\n",
    "decision tree confuse\n",
    "[[ 331  273]\n",
    " [ 329 1255]]\n",
    "decision tree- Accuracy: 72.49%\n",
    "decision tree - Precision: 82.13%\n",
    "decision tree - Recall: 79.23% \n",
    "\n",
    "neigh confuse\n",
    "[[ 282  322]\n",
    " [ 216 1368]]\n",
    "neigh- Accuracy: 75.41%\n",
    "neigh - Precision: 80.95%\n",
    "neigh - Recall: 86.36% \n",
    "\n",
    "MultilayerPerceptron confuse\n",
    "[[ 334  270]\n",
    " [ 225 1359]]\n",
    "MultilayerPerceptron- Accuracy: 77.38%\n",
    "MultilayerPerceptron - Precision: 83.43%\n",
    "MultilayerPerceptron - Recall: 85.80%\n",
    "\n",
    "multilayerPerceptron is best accuracy with a score of 0.7737659963436929\n",
    "multilayerPerceptron is best precision with a score of 0.8342541436464088\n",
    "knn is best recall with a score of 0.8636363636363636\n",
    "\n",
    "\n",
    "Par rapport a nos premières données, nous pouvons dire que tout nos résultats ont augmenter. Maintenant l'algo qui semble avoir mes meilleurs résultat est multilayerPerceptron pour l'accuracy et la précission. De plus nous avons dis que nous préfererions le critere pressision qui est ici le meilleur pour knn. pourtant le recall de multilayerPer.86.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b2dba41-2cd8-40b7-a380-c97666c6a1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables selectionnées pour axes 0\n",
      "0.3372542500143112 Home\n",
      "-0.34684304889618656 Time\n",
      "0.4049103338539558 Marital\n",
      "-0.4064454566103927 Records\n",
      "0.3109097478539244 Income\n",
      "0.2904013738758482 Debt\n",
      "0.2922408718192376 Status\n",
      "\n",
      "\n",
      "variables selectionnées pour axes 1\n",
      "0.3876246503271713 Age\n",
      "0.5945865365308064 Price\n",
      "0.5008414909714127 Status\n",
      "\n",
      "\n",
      "variables selectionnées pour axes 2\n",
      "-0.29563372846857827 Time\n",
      "-0.3237236229490682 Age\n",
      "0.4729330541603355 Expenses\n",
      "-0.34397178130839184 Income\n",
      "0.5348874184917022 Debt\n",
      "0.3105726492415841 Amount\n",
      "\n",
      "\n",
      "taux d'information conservé avec 3 dimension de la pca : 0.4430569133521197\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=3)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled_trans)\n",
    "X_train_pca_combinated = np.hstack((X_train_scaled_trans, X_train_pca))\n",
    "X_pca_train, X_pca_test, y_pca_train, y_pca_test = train_test_split(X_train_pca_combinated, label, test_size=0.5, random_state=1)\n",
    "\n",
    "def regle_de_seuil(data, pca):\n",
    "    axes_labels: List = []\n",
    "    for ind, axes in enumerate(pca.components_):\n",
    "        print(\"variables selectionnées pour axes\",ind)\n",
    "        label_res: str = \"\"\n",
    "        for mois, poid in zip(data.columns[1:], axes):\n",
    "            if abs(poid) >= 1/np.sqrt(len(data.columns[1:]) - 0.02):\n",
    "                print(poid, mois)\n",
    "                if(poid<0):\n",
    "                    label_res += '-'+mois+ \", \"\n",
    "                else:\n",
    "                    label_res += mois+ \", \"\n",
    "        axes_labels.append(\"description de l'axe \"+str(ind)+\": \"+label_res)\n",
    "        print(\"\\n\")\n",
    "    return axes_labels\n",
    "regle_de_seuil(data, pca)\n",
    "print(\"taux d'information conservé avec 3 dimension de la pca :\",sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c00d02-88ee-4842-ac37-076b6fba692c",
   "metadata": {},
   "source": [
    "Le taux d'information pour 3 variable est de : 0.44, nous pouvons déduire que le pca n'est pas pertinent pour 3 variables..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b7da3a4-4660-4e34-80f9-ccaeb3854168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.59063681 -0.50140677 -0.75576217]\n",
      " [-0.03365955  0.054126   -0.85098032]\n",
      " [ 3.05596106  1.58730331 -0.02089386]\n",
      " ...\n",
      " [-0.54993507 -1.78661792  1.25073621]\n",
      " [-2.82754524 -0.27174898  0.54129369]\n",
      " [ 0.7363677   0.85264748  0.54884128]]\n",
      "decision tree confuse\n",
      "[[ 335  269]\n",
      " [ 331 1253]]\n",
      "decision tree- Accuracy: 72.58%\n",
      "decision tree - Precision: 82.33%\n",
      "decision tree - Recall: 79.10% \n",
      "\n",
      "neigh confuse\n",
      "[[ 286  318]\n",
      " [ 207 1377]]\n",
      "neigh- Accuracy: 76.01%\n",
      "neigh - Precision: 81.24%\n",
      "neigh - Recall: 86.93% \n",
      "\n",
      "MultilayerPerceptron confuse\n",
      "[[ 321  283]\n",
      " [ 214 1370]]\n",
      "MultilayerPerceptron- Accuracy: 77.29%\n",
      "MultilayerPerceptron - Precision: 82.88%\n",
      "MultilayerPerceptron - Recall: 86.49% \n",
      "\n",
      "multilayerPerceptron is best accuracy with a score of 0.7728519195612431\n",
      "multilayerPerceptron is best precision with a score of 0.8287961282516636\n",
      "knn is best recall with a score of 0.8693181818181818\n"
     ]
    }
   ],
   "source": [
    "print (X_train_pca)\n",
    "X_pca_train, X_pca_test, y_pca_train, y_pca_test = train_test_split(X_train_pca_combinated, label, test_size=0.5, random_state=1)\n",
    "\n",
    "\n",
    "# X_norm_train, X_norm_test, y_norm_train, y_norm_test = train_test_split(X_train_pca, label, test_size=0.5, random_state=1)\n",
    "# arbre de désision\n",
    "clf_pca = tree.DecisionTreeClassifier()\n",
    "clf_pca.fit(X_pca_train, y_pca_train)\n",
    "# K voisin plus proche\n",
    "neigh_pca = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh_pca.fit(X_pca_train, y_pca_train)\n",
    "# MultilayerPerceptron\n",
    "mlp_model_pca = MLPClassifier(hidden_layer_sizes=(40, 20), random_state=1)\n",
    "mlp_model_pca.fit(X_pca_train, y_pca_train)\n",
    "\n",
    "clf_pca_predictions = clf_pca.predict(X_pca_test)\n",
    "clf_pca_res = show_predictions_result(clf_pca_predictions, y_pca_test, \"decision tree\")\n",
    "# Mesures de performance Pour l'abr de désision\n",
    "\n",
    "neigh_pca_predictions = neigh_pca.predict(X_pca_test)\n",
    "neigh_pca_res = show_predictions_result(neigh_pca_predictions, y_pca_test, \"neigh\")\n",
    "# Mesures de performance\n",
    "\n",
    "mlp_pca_predictions = mlp_model_pca.predict(X_pca_test)\n",
    "# Mesures de performance de MultilayerPerceptron\n",
    "mlp_pca_res = show_predictions_result(mlp_pca_predictions, y_pca_test, \"MultilayerPerceptron\")\n",
    "\n",
    "compare(clf_pca_res, neigh_pca_res, mlp_pca_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a972cf4-45e5-4618-9051-fd2cd20308c8",
   "metadata": {},
   "source": [
    "### Création de nouvelles variables caractéristiques par combinaisons linéaires des variables initiale\n",
    "\n",
    "| Metric                 | NORM                 | PCA                  |\n",
    "|------------------------|----------------------|----------------------|\n",
    "| **Decision Tree**      |                    |                      |\n",
    "| Accuracy               | 72.12%               | 72.58%               |\n",
    "| Precision              | 81.71%               | 82.33%               |\n",
    "| Recall                 | 79.23%               | 79.10%               |\n",
    "| **Neigh**              |                    |                      |\n",
    "| Accuracy               | 75.41%               | 76.01%               |\n",
    "| Precision              | 80.95%               | 81.24%               |\n",
    "| Recall                 | 86.36%               | 86.93%               |\n",
    "| **MultilayerPerceptron**|                   |                      |\n",
    "| Accuracy               | 77.38%               | 77.29%               |\n",
    "| Precision              | 83.43%               | 82.88%               |\n",
    "| Recall                 | 85.80%               | 86.49%               |\n",
    "\n",
    "\n",
    "Nous remarquons que de très faible changement. Glogbalement de petites augmentations, mais le meilleur algo MultilayerPerceptron subis de petites diminution.\n",
    "Il faut prendre l'algo MultilayerPerceptron normaliser mais sans ACP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43faf0d7-deb9-4ee2-b2d2-499c263bca0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c825fa-2b2c-439f-ae48-21879b9ff630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
